\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Learning the Way We Write: A Lightweight Automatic Adjustment of %offline
Neural Classifier of Handwritten Text to Individual Users}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Milan M.~Čugurović\\
  Department of Computer Science\\
  Faculty of Mathematics\\
  University of Belgrade\\
  Belgrade, 11000\\
  Serbia\\
  \texttt{milan\_cugurovic@math.rs} \\
  % examples of more authors
  \And
  Mladen Nikolić\\
  Department of Computer Science\\
  Faculty of Mathematics\\
  University of Belgrade\\
  Belgrade, 11000\\
  Serbia\\
  \texttt{nikolic@math.rs}
  \And
  Novak Novaković \\
  Microsoft Development Center Serbia\\
  Belgrade, 11000\\
  Serbia\\
  \texttt{novakn@microsoft.com}
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
  In this paper we propose a method of improving performance of a convolutional neural network (CNN) classifier of offline handwritten text by adjusting to the writing 
  style of an individual user. It does not require retraining or altering the base CNN classifier nor transfer of user's data to the server side, which results in 
  a lightweight privacy preserving method which does not require any specialized deep learning framework or hardware on the user's side. In the training phase, 
  performed on the server side, base CNN is trained and writing styles of different characters are defined by clustering in the feature space learned by the CNN. 
  In the adjustment phase, performed on the user side, user's writing style for different characters is determined by tracking their writing and corrections of 
  recognitions offered by the base CNN. This information is exploited by an ensemble of alternative k-nn classifiers with a lightweight Bayesian selection mechanism 
  which learns when to correct the base CNN. Essentially, the ensemble construction mechanism we propose can be understood as an automated error-space analysis of 
  the base CNN which is used for correction of its decision making.
  We tested our method on two relevant real-world datasets -- NIST Special Database 19 and ETH Zurich Deepwriting dataset. 
  We achieve up to 2.7\% improvement in classification accuracy and obtain state of the art results on these datasets.
\end{abstract}

\section{Introduction}
The problem of automatic recognition of offline handwritten characters is a very practical pattern recognition problem. Solutions to this problem are directly commercialized  by deployment in devices like tablets and smartphones. 
Accuracy of employed classifiers has significant impact to the user experience, which is of critical importance for the industry, since it can be a decisive factor for technology adoption. 
Therefore, motivated by its practical importance, its solutions were advanced both within academia and industry. 

The pioneering attempt to automatically recognize handwritten characters dates back to the 1950s \citep{leedham}.
After this initial attempt, different groups of researchers worked on this problem for decades \citep{hist1, hist2, hist3, hist4, hist5, plamondon}. 
Within the software and hardware limitations of the time, remarkable results were achieved.
In the last ten years, the intensive development of neural networks has led to a shift of boundaries in many areas,
including the area of offline handwriting recognition.
The results achieved by using convolutional neural networks exceed the results of all methods developed up to then \citep{cnnbest1, cnnbest3, cnnbest4, cnnbest2, cnnbest5}. 
Convolutional neural networks are also used as parts of complex systems that deal with the recognition of offline handwritten characters,
mainly as the powerful feature extractors. 
In such systems, the classification task is performed by alternative classifiers, who instead of working with the raw images, 
make their decisions based on a new set of attributes produced by the convolutional network \citep{cnnrelated1,cnnrelated2,cnnrelated3}. \marginpar{Da li je okej pojasnjenje?}

Although employment of convolutional neural networks has significantly increased offline handwriting classification accuracy, there is still plenty of room for progress. 
One way to improve handwriting recognition accuracy is to gather more data \citep{augment1, augment} or use better models so that the model performs better on average over a large population of users, 
which was the main direction of previous efforts. Another one is to enable model's adaptation to the specifics of handwriting of a single user
\citep{custom3, custom2, custom1, nist5, custom4, custom5}. 
In this paper, we presented a fast, alphabet-independent and scalable method that improves pretrained CNN without its retraining. Therefore, the improvement mechanism is easily deployed on user's device without requirements for specialized deep learning hardware or software. Also, no user data needs to be
transferred to the server. That way, potential issues regarding user privacy are avoided.
  
The proposed approach uses a so called base CNN which is trained on the server side, on a large dataset of labeled hand written character samples, as in most CNN based approaches. Besides the base CNN, for each character, several writing styles are defined by clustering, each cluster representing a distinct style. Clustering is performed over character embeddings learned by the CNN. The improvement method, running on the client side, can be understood as an automated error-space analysis of the base CNN, which is used for correction of its decision making. In the adjustment phase, user's writing style for different characters is determined by tracking their writing and corrections of recognitions offered by the base CNN. This information is exploited by an ensemble of alternative k-nn classifiers with a lightweight Bayesian selection mechanism which learns when to correct the base CNN.

The main points of our work are the following:
\begin{itemize}
\item We propose a new method for adjusting a CNN based system for offline hand writing recognition to the writing style of a specific user and achieve up to 2.7\% improvement in classification accuracy on two relevant real-world datasets.
\item The proposed method is lightweight on the clinet side and does not require specialized deep learning hardware or libraries.
\item The proposed method is privacy preserving, as it does not require any transfer of user's data to the server.
\end{itemize}

The rest of the paper is organized as follows. Section 2 describes related work. The proposed method is fully described in Section 3. Section 4 provides experimental results. Conclusions are drawn in Section 5.

\section{Related work}  % Da li treba da izgleda "malo standardnije"?

This section provides background about improving offline handwritten character recognition.
In addition, an overview of the results achieved so far on the datasets we use is given.
% Izbaceni delovi:
% Due to their properties, convolutional neural networks are one of the leading classification mechanisms by which many problems of computer vision are solved.
% In the area of offline handwriting character recognition CNN achieves some of the state-of-the-art results.
% In terms of publications related to improving offline handwritten character recognition,
% there are only a few of them that tend to improve the offline handwritten text classifier. 
%However, the method presented in it doesn't focus on the writing style of the individual users, which is the basic idea that motivated our improvement method. 

Adapting classifiers to each of the individual users is a relatively old idea that can be applied not only to CNNs but to other offline handwritten text classifiers 
as well as on online handwritten text recognizers \citep{adapt3, adapt2, adapt7}. 
The most common approach to writer adaptation for Gaussian Mixture Hidden Markov Models is the Maximum Likelihood Regression, 
which was used to fine-tune the means and variances used in HMMs \citep{adapt1, adapt4}. 
\citet{adapt6} show that they can adapt the HMM-based recognition engine even when using less than $30$ word samples. 
They estimated that $200$ words was the lower bound for training from scratch (solely on the writers words) to produce better results than adjusting a pretrained model. 
\citep{adapt5} propose a method of prototype integration for writer adaptation in which they use an iterative bootstrapping model that adapts a writer-independent model to a writer-dependent one. 
They also used a conventional HMM recognition engine. 

\citet{nosary1, nosary2, nosary3} propose the adaptation principle which was based on automatic learning of the handwriting graphical characteristics, 
which was inspired by human contextual effects that influence the visual word recognition and which focus on the redundant elementary patterns extracted 
from each user's handwriting. They store their system recognition output and use it as an additional training data at a later stage, as recognition progressed. 

The aforementioned publications \citet{custom1, nist5} propose a smaller augmenting network that can be re-trained on user device using his own data. %Using the smaller engine allows customization of large base network without losing generality.
A coarse-grained reconfigurable array processor was used to reduce the energy required for network retraining.
In addition to this work, there is a whole series of papers dealing with the effective retraining of the base neural classifier \citep{custom2, custom4}. 

Recently, ways have been developed to adapt the neural network classifiers to the handwriting of the individual users. 
\citet{cnn0} propose a style transfer mapping (STM) as a closed-form writer-specific and class-independent feature transformation which the data of different writers project onto a style-free space 
with a goal to reduce the impact of handwriting style variation among different persons. 
\citet{cnn1, cnn2} combine the aforementioned STM with the base CNN classifier in an unsupervised fashion by using pseudo-labels which were generated by the base classifier. 
They integrate the previous by adding an adaptation layer to pretrained CNN, which weights was tuned during the adaptation process. 
For training the adaptation layer, they used the modified STM learning technology. 
\citet{cnn3} propose innovative writer code based adaptation of the DNN-HMM for furthermore accuracy improuvement via a personalized recognizer. 
\citet{cnn4} introduced a novel adversarial feature learning model (AFL) which tends to improve handwritten character recognizer 
by focusing on the features of standard printed and handwritten characters. 

A standard way to improve the performance of a set of classifiers is to integrate them using bagging, voting and other stacking techniques.
Here we mention papers of this type, although this is not essentially a work on improving a particular classifier \citep{imp3, imp1, imp2}.

% NIST rezultati
On the NIST dataset, several independent groups of researchers have published papers with different classification algorithms.
A committee of seven CNNs obtains a precision of $88.12\%$ \citep{nist1}.
For committee forming training is conducted both on the original and six preprocessed datasets
(preprocessing is motivated by the purpose of handling different aspect ratios of the handwritten characters).
Using a multicolumn deep neural network a precision of $88.37\%$ was achieved \citep{nist2}.
This architecture was motivated by biological ideas, forcing sparsely connected neural layers like in a mammal's brain.
Combining a convolutional neural network as a feature extractor and (linear) support vector machines as a classifier,
an efficient method for improving the convolutional neural network is presented \citep{nist3}.
By using the methods of k nearest neighbors, bagging and random forest classifier, a cheap feature selection algorithm was developed,
and precision around $75\%$ was achieved \citep{nist4}.
Recently, on-device user customization of CNN which implies retrain of smaller augmenting network is proposed,
showing a 3.5-fold reduction of the prediction error after doing user customization \citep{nist5}.

% Deepwriting rezultati
There are currently no published results of the classification problem on the Deepwriting dataset.

\section{Method}

This section describes the proposed method for improving the CNN classifier of offline handwritten text.
Note that while the idea behind the learning users handwriting style is rather simple, because of the complexity of the proposed method, we first give it an overview.

\subsection{Method overview}

The basic idea is to distinguish the reference modes %pattern
of writing for each character.
This is motivated by the intuition that each character can be written in finite significantly different ways.
Separation the different writing styles of each character was achieved by clustering.
Thus, for each of the characters, several clusters were obtained representing aggregated different writing styles of that character.
This tends to separate the different writing styles of each of them.

When the proposed method is used, on the application set, for each user a set of characters that he or she authored is divided into two sets: an adaptation set and test set.
On the adaptation set, we detect the user's writing style by memorizing his individual characters writing patterns.
On the test set, the proposed method makes predictions by taking into account the user's writing style.

A sketch of the proposed method, which will be explained in more detail in the following sections, is given with:
\begin{itemize}
  \item Images from the training set for the base classifier and validation set for the base classifier are grouped by labels (characters) and clustered within each group.
  These clusters represent the main writing styles for each of the characters.
  \item For each author of the application set:
  \begin{itemize}
    \item The set of his images is stratified into an adaptation set and test set.
    \item In the adaptation set, for each of the characters, the most similar writing styles (from already defined writing styles) are being identified, which makes the author's writing history.
    \item Alternative classifiers (K nearest neighbor method) and their confidence vectors are created on the adaptation set.
    \item For every instance of a test set (where the proposed method is used):
      \begin{itemize}
        \item In addition to the base CNN prediction, alternative classifier predictions are also calculated.
        \item Based on the confidence vectors of all the classifiers, the prediction of the most reliable of them is selected.
        \item For each classifier, using the correct and its predicted labels, its confidence vector is updated.
      \end{itemize}
  \end{itemize}
\end{itemize}
A more detailed description of each step, as well as an explanation of terms such as writing history and the classifier confidence vector are given in more detail in the following sections.

\subsection{Clustering individual character writing styles}

The first step of the proposed method is to cluster the images within sets with the same labels.
The main idea that motivated this step of the improvement method is the assumption that although there are variations in the ways of writing a particular character by individual authors, nevertheless this set of variations is finite.
%Another motive for using clustering by instances is the fact that an individual author when writing different characters, will never write two completely identical.
%Any newly written character will be a small modification of his way of writing.
%By averaging such characters, we expect that variation in writing will annul, and this again leads to a crystallization of the style (the way in which the author writes that character).
We clustered images of the training set for the base classifier and validation set for the base classifier.

We used the output of the next-to-last layer of the base CNN classifier as a set of attributes that characterize a handwritten character image \citep{nexttolast}.
The next-to-last layer of the neural network gives representations of the input images in some new attribute space in which we expect the different characters to be well separated,
while the last layer of the neural network only plays the role of labeling over those representations.
Therefore, the last layer of the network is cut off, and for the set of attributes that describe the image from the input we used the values of the neurons of the next-to-last CNN layer \citep{style1, style2}.

Due to the nature of our data, we used the K-means clustering algorithm \citep{kmeans}.
Cluster centroids will represent the writing styles of the clustered character.
The number of clusters for each label is given by the formula:
\begin{equation}
  k = \min(30, 1+\max(n/1000, 4))
\end{equation}
where $n$ represents the number of images that were clustered.
Previous heuristics were determined experimentally, with respect to the used datasets.
The quality of clustering was evaluated by using the K nearest neighbor algorithm (KNN) which was trained on the obtained centroids.
Euclidean metric was used for clustering as well as for its evaluation.
The number of neighbors in this evaluation belonged to the set $\{1,2,3,4,5,7,8,9,10,15\}$.
Clustering quality was evaluated based on the highest precision of the KNN method for various values of $k$.
The motivation for this approach is the fact that the presented improvement method will, in the following, use the KNN algorithm in order to find the character most similar to the current one within the writing history.

\subsection{Creating a user writing history}

After clustering, for each character (label) a certain number of its characteristic writing styles were selected (centroids of clusters), which were represented by vectors of appropriate dimensions
(the dimension of the vector is equal to the number of neurons in the next-to-last layer of the base CNN).
The second part of the method, which is described in this section, relates to the model use pahse.

Like we said before, images each of individual users from the application set are stratified into two sets: an adaptation set and a test set. % Vec sam ovo rekao u overviewu metoda, brisati?
In the adaptation set, the method of improvement creates the writing history of each individual user.
Based on the writing history, the base classifier will be improved during its usage.
Based on the writing history, it is also necessary to conclude when the base classifier for the current user and his/her writing style make a mistake, and how to correct those mistakes.

The process of creating a user's writing history consists of the following.
For the input image, we focus on the correct label as well as the label provided by the base classifier.
Based on the output of the next-to-last layer of CNN classifier, we find the closest cluster among the clusters that correspond to the correct label (among the various writing styles of that character).
We used the Euclidean metric for the search. % fix
This search attempts to identify the user's character's writing style with some of the predefined styles.

The writing history for each ordered pair \textit{(predicted label, correct label)} remembers the average of the centroids closest to the output vector of the next-to-last CNN layer.
The interpretation of this is as follows: when the base CNN classifier predicts % provide?
 a \textit{predicted label} for a picture showing the \textit{correct label},
on the basis of the output of the next-to-last layer of the base CNN, this ordered pair is assigned an appropriate cluster (a way of writing the correct character).
Clusters assigned to the same pair may not always be identical.
We consider their average to be a writing style for a particular character of a particular user.

\subsection{Using user writing history}

After creating user writing history, it is necessary to make decisions on which cases to trust the base classifier, and when to modify its predictions and change them based on the method K closest neighbors
which was trained on the mentioned user writing history.
The method we present, on the history of writing do not create one KNN model for correction, but a series of KNN models, where $k$ taking values $2, 4, 6, 8$ and $10$.

Based on the users writing history, a confidence vector of the base classifier and a confidence vectors of the KNN methods are created for each of the previous values of the $k$ parameter.
The classifier confidence vector is an array of beta distribution expectations, which evaluate the reliability of the classifier when predicting each of the labels.
One beta distribution is created for each label $l$.
The degree of confidence of the classifier for label $l$ is represented by the mathematical expectation of the created beta distribution.
This expectation is a function of parameters $\alpha$  and $\beta$.
For each image of the adaptation set for which the classifier predicts label $l$, the parameters $\alpha$  and $\beta$ of the beta distribution are updated, depending on whether the prediction is correct or not.
Correct predictions increase the value of parameter $\beta$
(thus increasing the mathematical expectation of beta distribution, that is the degree of confidence in the classifier when it predicts the label $l$),
while negative predictions increase the value of parameter $\alpha$
(thus reducing the mathematical expectation of beta distribution).

The confidence vector of each of the classifiers is updated based on the original labels and their predictions for the images of the adaptation set.
The KNNs training set is a subset of the user's writing history, consisting of vectors and their original labels of all writing history pairs for
which the \textit{predicted label} equals the current prediction of the base classifier.

In doing so, we created appropriate confidence vectors at the adaptation set, and we evaluate the reliability of both the base classifier and the k nearest-neighbor methods, for different values of $k$.
Based on that, we determine which one classifier to trust during model use (we believe the classifier that for the given image has the highest reliability on the character he predicts).

When used, in a test set, instead of just one prediction that a base CNN classifier gives us, we get multiple of them simultaneously.
For each image, based on the predictions of the base CNN classifier and the author's writing history, we obtain predictions of nearest neighbor methods for various values of $k$.
If the base classifier predicts label $l$, within the writing history, we look at the style vectors of all the pairs \textit{(predicted label, correct label)} for which the \textit{predicted label} is precisely $l$.
Using these vectors, for each value of $k$, we make the decision by the KNN method with respect to the correct labels.
Based on the confidence vector of the base classifier (we use the value of expectation that relates to the prediction of label $l$), and based on confidence vectors of KNN methods for various values of $k$
(we use the values of expectations related to their predicted labels) we decide which character to predict.

After the evaluation of the individual image, we update the parameters of the corresponding statistics for each of the classifiers.
Depending on whether the classifier predicted the correct label or not, we are updating the beta distribution parameters which are used to evaluate the classifier's reliability on the predicted character.
This corresponds to a real-world application, in which the user would have corrected his just-written character if the handwriting recognition application had failed to successfully classify it,
and the application would have information about the correct label.
In the absence of a correction, the model is sure to have successfully classified the character just processed.
With this method of execution, the presented method achieves better results over longer, because the degree of its reliability is directly proportional to the size of the user writing history.
This is a great advantage when using the presented improvement model in practice.

\section{Evaluation}

In this section, the results of the proposed method will be discussed.
We carried out the experiments on two public databases: NIST Special Database 19 (American National Institute of Standards and Technology) and
Deepwriting Dataset (ETH Zurich). % Da li ovde da ih citiram ili dole u pasusima sa njihovim detaljima?
In independent evaluations, our improvement method behaves very stable, and in the first of them increases the accuracy of the base classifier by approximately $2.3-2.5\%$.
In the second dataset, the increase in precision of the base classifier is greater than $2.7\%$.

In addition to significantly increasing the precision of base CNN, other qualities of the proposed method should be considered.
See Section 5 for detailed information. % Da izbacim ovu najavu? Nekako se prirodno nadoveze na one info o povecanju?

Specifically, our source code was released on \url{https://github.com/MilanCugur/NNClassifierImprove}.

\subsection{Used datasets}

The idea of this paper is based on the adaptation of the base classifier to the individual users,
and therefore a dataset with a large amount of data for each character each of individual users is needed
(in order to learn the specifics that characterize each of them).
This turns out to be a very big limitation and only a small number of datasets satisfy this property.
As discussed in the introduction, the datasets NIST and Deepwriting were used in the paper.
More information about them is given below.

\subsubsection{NIST Special Database 19}

The NIST Special Database 19 \citep{nist} contains handwritten characters collected from slightly less than $3600$ authors in the approximately $810,000$ images,
scanned from the appropriate forms, along with the corresponding labels.
In addition, the database contains reference forms for possible further data collection and software tools for working with them.

The first version of this database was published as CD-ROM \citep{nistv0}, and then re-released using a modern file format \citep{nist}.
The dataset contains binarized images created from $3669$ forms and contains segmented, manually classified handwritten letters and numbers.
These characters are presented as monochromatic images of $128 \times 128$ resolution while labeled with one of the 62 ASCII classes corresponding to the English alphabet characters and Arabic numerals.
The database is structured in five different ways, hierarchically organized according to different criteria.
In this paper, the data used is structured by appropriate authors or classes.

Detailed information about dataset are given in the table \ref{table1}.

\subsubsection{ETH Zurich Deepwriting Database}

The ETH Zurich Deepwriting Database\citep{deepwriting} contains handwritten text annotated at the level of sentences, words and individual characters.
About $300$ authors wrote around $400,000$ characters.

It was created as an upgrade of the existing IAM Handwriting database \citep{iam}, which is here labeled at the level of individual characters.
In addition to the data in this database, new data were collected with the help of web tools developed for iPod Pro.
New authors, 94 of them, wrote the text Lancaster-Oslo-Bergen (LOB) \citep{lob}, since the authors in IAM Database also wrote that text.
Characters in the database were initially given as online handwritten characters.
In addition to the data, the authors provided software that could be transform database to offline variant.
Based on the software provided, offline character images were created.
Images in datased are labeled with $70$ labels, which are lowercase and uppercase English alphabets, Arabic numerals, and additionally characters
' . , - ( ) / and space.

Detailed information about dataset are given in the table \ref{table1}.

\begin{table}[h!]
  \caption{Detailed information about used datasets}
  \label{table1}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{}{Dataset} & Dataset \\
    \cmidrule(r){2-3}
    Property & NIST    & Deepwriting \\
    \midrule
    Number of writers &  $3596$  & $294$     \\
    Number of images & $814,255$ & $406,956$      \\
    Number of labels & $62$ & $70$ \\
    Average number of images per user & $226.43$ & $1384.20$ \\
    Average number of images per user $\times$ label & $3.65$ & $19.77$ \\
    \bottomrule
  \end{tabular}
\end{table}

% Da li je potrebno objasniti zasto smo uspesni na NISTU a samo 3.65 je prosek?
\subsection{Images preprocessing}

On both datasets preprocessing was inspired by the famous MNIST dataset \citep{mnist} and it was modeled on the work that describes the creation of the EMNIST dataset \citep{emnist}.
In the mentioned paper, the conversion process consists of the following steps, in sequence:
blurring images using Gaussian filter, cropping the character from the image, centering the previously extracted character, and resizing the image into a $28 \times 28$ pixel frame.
Unlike the mentioned publication, we first cut the original image whereby the size of the margin around the character is set to one pixel.
After that, a Gaussian filter is applied to the cropped character (the standard deviation of the Gaussian kernel is set to $1$).

After cropping and blurring, the character is centered in a square frame.
When centering, it is important to emphasize that the aspect ratio of the extracted character is preserved, because we expand the shorter dimension of the image with the empty space.
This transformation from an image of dimension $w\times h$ creates an image of dimension $max\{w, h\}\times max\{w, h\}$.
The original character height to width ratio is one of the specifics of the handwriting, so losing this aspect ratio would be a loosing of information.
Finally, a square image of dimension $max\{w, h\}\times max\{w, h\}$,
where $w$ and $h$ are original image width and height respectively, is converted to $28 \times 28$ pixels.
This conversion was done using bi-cubic interpolation.

Datasets, created as described, are available in Zip format (as addition to this paper).

\subsection{Datasets split}

The dataset is divided into a training set for the base classifier, a validation set for the base classifier and an application set.
Further, the application set is divided into two parts: an adaptation set and a test set.
When splitting the data into subsets, all images of one author belong to the same set.
Considering the idea of the proposed approach, which implies learning the specifics of the author's handwriting style's on each individual character,
a significant number of images are required for each individual character of each individual user.
As the original dataset is quite unbalanced (in terms of the character count of each user), the division was made so that those who wrote more were transferred to the application set,
while users with less written characters remained in the training and validation set of the base classifier.

Approximately ten percent of all images are in the application set, while the remaining images are divided into training and validation sets for the base classifier
such that the number of authors in them is in a ratio of $9 : 1$.
90\% of the images of each author of the application set make his adaptation set, while the remaining 10\% of the images makes his test set.

Detailed information about datasets splits are given in the table \ref{table2}.

\begin{table}[h!]
  \caption{Breakdown of the number of samples in created splits}
  \label{table2}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{}{Dataset} & Dataset \\
    \cmidrule(r){2-3}
    Property & NIST    & Deepwriting \\
    \midrule
    No. of writers in train set for the base classifier &  $3057$  & $242$     \\
    No. of writers in validation set for the base classifier & $339$ & $27$      \\
    No. of writers in adaptation set & $200$ & $70$ \\
    No. of images in train set for the base classifier &  $659,541$  & $298,313$     \\
    No. of images in validation set for the base classifier & $73,209$ & $33,484$      \\
    No. of images in adaptation set & $81,505$ & $37,658$ \\
    \bottomrule
  \end{tabular}
\end{table}

It is important to point out that the NIST dataset is extremely unbalanced, in the sense that for each individual user there are many more images that are
labeled with some of the numbers than with other characters.
Despite this, the improved method presented in this paper has proven to be very successful.
On this basis, it is realistic to expect an even better system behavior in the case of favorable datasets, or in realistic implementation.

\subsection{Base CNN classificator}

The base classifier whose improvement is presented in this paper is a convolutional neural network based on the \textit{Caffe} architecture \citep{caffe}.

\subsubsection{Architecture}

Our base neural network is trained on images of size $28 \times 28$, while its output has a Softmax function or a layer that predicts as a class, in the case of the NIST database one of $62$ labels,
and in the case of the Deepwriting dataset one of $70$ labels.
The base neural classifiers created for the two datasets considered in this paper differ only in the number of neurons of the last layer.

The convolutional part of the neural network consists of three successive blocks, each consisting of two consecutive convolutions, followed by Batch normalization layer,
which is again followed by an Aggregation layer.
The number of filters in the convolutional layers increases monotonically by blocks.
Convolutions use a Rectified linear unit (ReLU) as an activation function.
Aggregation is performed using the Max-pooling function, retaining the same image dimensions after aggregation.
The convolutional part is followed by a fully connected block, which uses Dropout regularization.
The complete network contains more than $350,000$ parameters.

\subsubsection{Training and results}

The basic neural classifier was trained by the Adam optimization method with parameters $\beta_1=0.9$, $\beta_2=0.999$ and with a learning rate of $0.001$.

In the case of the NIST dataset, the base classifier was trained in $35$ epochs in three series of $20$, $10$ and $5$ epochs each.
Each successive series uses a larger batch size.
This kind of training is inspired by the actual work in this area \citep{lrbs}.
The results achieved by the base classifier on the NIST dataset are $88.36\%$, $87.39\%$ and $87.35\%$ respectively on the training set for the base classifier, validation set for the base classifier and application set.

The base classifier on the Deepwriting dataset was trained for 20 epochs, after which training was stopped by the early stopping technique.
The results of such a trained classifier are are $87.65\%$, $83.12\%$ and $82.12\%$ respectively on the training set for the base classifier, validation set for the base classifier and application set.

\subsection{Evaluation results}

The evaluation method includes both data stratification and resampling techniques.

The stratified data split of each of the users from the application set into his adaptation and testing sets are of great importance.
The stratification is performed with respect to the corresponding labels, thereby achieving approximately equal distribution of the labels in the adaptation set and in the test set.
This avoids the situation of character occurrence in evaluation, where the corresponding label was not seen in the user writing history
(in such a scenario, our model cannot give an improvement but behaves the same as a base classifier).
This situation, however, cannot be completely eliminated, but its likelihood can be reduced.
This partially eliminates the bad properties of the datasets we work with.
In the case of an ideal dataset, or real use scenario, stratification would not be required.
The stratified split of the application set images was performed by sending $90\%$ of the images to the adaptation set, while $10\%$ of them ended up in the test set.

In the evaluation, resampling is used so that for an arbitrary user precision is not only calculated once on the application set, but the same procedure is repeated
independently $n$\footnote{where $n$ represents the resampling parameter} times,
each time with a stratified split of data into a section intended to create a user writing history and a section intended for the evaluation.
% Da li treba motivisati zasto se radi resampling?

The presented method goes beyond the results reported so far on the NIST Special Database 19, using the transformation of images into resolution $28 \times 28$ promoted by the MNIST and EMNIST data sets.
It is important to note that the data split we used does not follow the recommended data partition for the training and test set of the NIST dataset, due to the specificity of the proposed method
(the division used in the evaluation create application set of those authors who have written the most characters, cause of the great imbalance and difference
in the number of written characters between the authors of the NIST dataset and
the fact that the proposed method requires a certain amount of data to learn information about the writing style of an individual user).
Therefore, there is no easy way to directly compare our results with the previous ones. % Therefore, there is no easy way to directly compare our work with previous, and therefore we don't claim that the proposed model is better than all previous ones, (we are just showing that our results are in the best rankings so far).
The presented improvement method achieves a precision of $89.60\%$, which outperforms the previously published results.
This result was achieved by improuving a base network whose results were worse than the best published on NIST so far
(by increasing the accuracy of the base network for $2.4\%$, we have exceeded the best published results on this dataset so far).
% Prvo kazem nismo lako uporedivi, odmah posle mi smo bolji, da li je to okej?

On the Deepwriting dataset, there are no published results of the classification problem so far, and therefore our results are state of the art.
The presented improuvement method increases the precision of the base classifier for $2.72\%$ thereby achieving a precision of $84.77\%$.

More detailed evaluation results (whereby the number of resamples was set to $10$) are given in the paragraphs below.

During the evaluation, for each user of the application set, we monitored the precision of the base classifier as well as the precision of the improvement method.
The number/percentage of images on which the base classifier correctly predicted the labels, the number/percentage of images on which the improuvement method gave the correct labels,
and the number/percentage of images where at least one of them was correct were monitored.
The last one is included for the purpose of assessing the quality of improvement method,
which is a marginal case or scenario that could happen if we in any time knows which classifier to believe,
whether the the base or one of the methods of the K nearest neighbor simultaneously performed with him.
Previous represents the upper limit for the improvement method quality.

In the case of NIST dataset the model was tested on $81,685$ images written by $200$ different authors.
The precision of the base classifier on them is $87.24\%$, while the precision achieved by the improvement method on the same set is equal to $89.60\%$.
The upper limit of precision achievable by the our method is $91.42\%$.
There are $24$ authors on whose the base classifier performs better than the presented improvement method, on $9$ authors they have the same performance,
while as many as on $167$ of them a new improvement method has better performance than the base CNN.
In the performed evaluation, the improvement method gives an increase in the precision of the base classifier of $2.36\%$.
Theoretically, the highest possible increase in precision (by this evaluation) is $4.18\%$.

In the case of the Deepwriting dataset we tested our model on $37,672$ images, which was written by $25$ different authors.
The base classifier achieves an accuracy of $82.05\%$, improvement method of $84.77\%$, while the upper limit of improvement method is $86.38\%$.
Our model increases the precision of the base classifier by $2.72\%$, while in theory the largest possible increase in precision equals $4.33\%$.
On the images of $24$ authors, our method achieves greater precision than the base network, while on the images of one author, the base network achieves greater precision.

%\subsection{Comparison with relevant papers}
% Ovo možda u related works?
% Ali kako onda uporediti?
\section{Conclusion}

% O nekim stvarima tipa alphabet-independent, stability, scalability etc. pricam prvi put u zakljucku.
% Da li ih pisati ranije i gre?
In this paper, we present a conceptually simple yet powerful method, by which we were able to increase the precision of the base neural network up to $2.7\%$.
The proposed method exhibits many positive properties, such as stability
(in terms of showing good performance with multiple independent evaluations) and scalability (in terms of improving performance with more data),
which is very important for real application because then we are expecting a really large amount of information about the handwriting of the device user.
The presented method is also alphabet independent, which greatly expands its application domain.
In this paper, we have been able to improve the neural network without re-training it by creating a system that adapts to it the right way,
learning its mistakes and making the right predictions.
Therefore, our system is very fast and does not require powerful graphics cards for its execution,
which makes it suitable for installation on smaller devices, which again qualifies it for practical use.

The main contribution of this paper is the development of a new method that uses basic algorithms and machine learning techniques such as K-means clustering
and K nearest neighbor classification, which outperforms the performance of leading classifiers of offline handwritten text.
The proposed model of improvement at the time of use of the base classifier provides prediction by combining the prediction of the base classifier
with the predictions of a series of auxiliary classifiers.
This delegates decision making, thus creating space to isolate the handwriting style of each user.
It is important to note that our work is based on learning the writing style of individual users and is the first work of this kind.
No work before ours attempts to improve the performance of an offline handwritten text classifier by focusing on the writing style itself.
% Mozda istaci ranije prethodne dve recenice?

Future research may include meta-learning and few-shot learning techniques to prepare a base classifier for quick adaptation to each new user.

% Negde gore komentarisati kako i zasto smo koritili Beta raspodelu da kazem zbog konjigate priors i sl.?
% Citirati rad sem iz NI?
\small

\begin{thebibliography}{99}
  \bibitem[Patrick et al. (2016)]{nist} Patrick, Grother \ \&  Kayee, Hanaoka\ (2016) NIST Special Database 19 Handprinted Forms and Characters, 2nd Edition

  \bibitem[Aksan et al. (2018)]{deepwriting} Aksan, Emre \ \& Pece, Fabrizio \ \& Hilliges, Otmar\ (2018) DeepWriting: Making Digital Ink Editable via Deep Generative Modeling
  In {\itshape SIGCHI Conference on Human Factors in Computing Systems}: CHI '18, ACM, New York, NY, USA

  \bibitem[C.G. Leedham (1994)]{leedham} C.G. Leedham \ (1994) Historical perspectives of handwriting recognition systems
  In {\itshape IEE Colloquium on Handwriting and Pen-Based Input}: 1-3

  \bibitem[Dimond (1957)]{hist1} T. L. Dimond. \ (1957) \ Devices for reading handwritten characters.  In papers and discussions presented at the December 9-13, 1957, eastern joint computer conference: Computers with deadlines to meet (IRE-ACM-AIEE '57 (Eastern)). ACM, New York, NY, USA,  232-237. DOI=http://dx.doi.org/10.1145/1457720.1457765

  \bibitem[Eden (1962)]{hist2} M. Eden \ (1962) \ "Handwriting and pattern recognition," in IRE Transactions on Information Theory, vol. 8, no. 2, pp. 160-166, February 1962. doi: 10.1109\/TIT.1962.1057695
  
  \bibitem[Crane et al. (1975)]{hist3} Crane, Hewitt David \& Robert Ellis Savoie \ (1975) "Special pen and system for handwriting recognition." U.S. Patent No. 3,906,444. 16 Sep. 1975.

  \bibitem[Davis et al. (1986)]{hist4} Davis, Robert H. \& J. Lyall \ (1986) \ "Recognition of handwritten characters—a review." Image and Vision Computing 4.4: 208-218.

  \bibitem[Bozinovic et al. (1989)]{hist5} Bozinovic, Radmilo M. \& Sargur N. Srihari. \ (1989) \ "Off-line cursive script word recognition." IEEE Transactions on Pattern Analysis and Machine Intelligence 1: 68-83.

  \bibitem[Plamondon et al. (2000)]{plamondon} Plamondon, R., \ \& Srihari, S.N.\ (2000).
  On-Line and Off-Line Handwriting Recognition: A Comprehensive Survey. IEEE Trans. Pattern Anal. Mach. Intell., 22, 63-84.

  \bibitem[Srihari et al. (2006)]{cnnbest1} Srihari, Sargur N., \& Xuanshen Yang \& and Gregory R. Ball. \ (2006) “Offline Chinese Handwriting Recognition : A Survey.”

  \bibitem[Pavarez et al. (2013)]{cnnbest2} Parvez, Mohammad \&  Mahmoud, Sabri. \ (2013). Offline Arabic Handwritten Text Recognition: A Survey. ACM Computing Surveys (CSUR). 45. 10.1145/2431211.2431222.

  \bibitem[Awaida et al. (2012)]{cnnbest3} Awaida, Sameh \& Mahmoud, Sabri. \ (2012). State of the art in off-line writer identification of handwritten text and survey of writer identification of Arabic text. Educational Research and Reviews. 7. 10.5897/ERR11.303.

  \bibitem[Yuan et al. (2012)]{cnnbest4} Yuan, A. \& Bai, G. \& Jiao, L. \& Liu, Y. \ (2012, March) \ Offline handwritten English character recognition based on convolutional neural network. In 2012 10th IAPR International Workshop on Document Analysis Systems (pp. 125-129). IEEE.

  \bibitem[Poznanski et al. (2016)]{cnnbest5} Poznanski, A. \& Wolf, L. \ (2016) Cnn-n-gram for handwriting word recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2305-2314).

  \bibitem[Fiel et al. (2015)]{nexttolast} Fiel, Stefan \& Sablatnig, Robert. \ (2015). Writer Identification and Retrieval Using a Convolutional Neural Network. 26-37. 10.1007/978-3-319-23117-4\_3.

  \bibitem[Gatys et al. (2015)]{style1} Gatys, Leon \& Ecker, Alexander \& Bethge, Matthias. \ (2015). A Neural Algorithm of Artistic Style. arXiv. 10.1167/16.12.326.

  \bibitem[Johnson et al. (2016)]{style2} Johnson, Justin \& Alahi, Alexandre \& Li, Fei Fei. \ (2016). Perceptual Losses for Real-Time Style Transfer and Super-Resolution.

  \bibitem[Hartigan et al. (1979)]{kmeans} Hartigan, J. A., \& M. A. Wong. \ (1979) "Algorithm AS 136: A K-Means Clustering Algorithm." Journal of the Royal Statistical Society. Series C (Applied Statistics) 28, no. 1: 100-08. doi:10.2307/2346830.

  \bibitem[Grother (1995)]{nistv0} Grother, Patrick J. \ (1995) "NIST special database 19." Handprinted forms and characters database, National Institute of Standards and Technology.

  \bibitem[Marti et al. (2002)]{iam} Marti, U. V., \& Bunke, H. \ (2002). The IAM-database: an English sentence database for offline handwriting recognition. International Journal on Document Analysis and Recognition, 5(1), 39-46.

  \bibitem[Garside et al. (1986)]{lob} Garside Rodger \& Stig Johhanson \& Atwel Eric \& Leech Georey \ (1986) "The tagged LOB Corpus: User's manual"

  \bibitem[Cohen et al. (2017)]{emnist} Cohen, G. \& Afshar, S. \& Tapson, J. \& van Schaik, A. \ (2017). "EMNIST: an extension of MNIST to handwritten letters." arXiv preprint arXiv:1702.05373.

  \bibitem[LeCun (1998)]{mnist} LeCun, Y. \ (1998). "The MNIST database of handwritten digits." http://yann.lecun.com/exdb/mnist/.

  \bibitem[Jia et al. (2014)]{caffe} Jia, Y. \& Shelhamer, E.\& Donahue, J. \& Karayev, S. \& Long, J.\& Girshick, R., ... \& Darrell, T. \ (2014, November). "Caffe: Convolutional architecture for fast feature embedding." In Proceedings of the 22nd ACM international conference on Multimedia (pp. 675-678). ACM.

  \bibitem[Smith et al. (2017)]{lrbs} Smith, S. L. \& Kindermans, P. J. \& Ying, C. \& Le, Q. V. \ (2017). "Don't decay the learning rate, increase the batch size." arXiv preprint arXiv:1711.00489.

  % Related works publications:
  \bibitem[Ciresan et al. (2011)]{nist1} Ciresan, D.C. \& Meier, Ueli \& Gambardella, L.M. \& Schmidhuber, Juergen. \ (2011). "Convolutional Neural Network Committees For Handwritten Character Classification. Proceedings of the International Conference on Document Analysis and Recognition", ICDAR. 1135 - 1139. 10.1109\/ICDAR.2011.229.

  \bibitem[Ciresan et al. (2012)]{nist2} Cireşan, Dan \& Meier, Ueli \& Schmidhuber, Juergen. \ (2012). "Multi-column Deep Neural Networks for Image Classification." Proceedings  CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 10.1109\/CVPR.2012.6248110.

  \bibitem[Darmatasia et al. (2017)]{nist3} Palehai, Darmatasia \& Fanany, Mohamad Ivan. \ (2017). "Handwriting Recognition on Form Document Using Convolutional Neural Network and Support Vector Machines (CNN-SVM)." 10.1109\/ICoICT.2017.8074699.

  \bibitem[Cilia et al. (2018)]{nist4} Cilia, Nicole \& De Stefano, Claudio \& Fontanella, Francesco \& Scotto di Freca, Alessandra. (2018). "A ranking-based feature selection approach for handwritten character recognition." Pattern Recognition Letters. 10.1016\/j.patrec.2018.04.007.

  \bibitem[Barend et al. (2019)]{nist5} Harris, Barend \& Bae, Inpyo \& Egger, Bernhard. \ (2018). "Architectures and algorithms for on-device user customization of CNNs." Integration. 10.1016\/j.vlsi.2018.11.001.

  \bibitem[Durjoy et al. (2015)]{cnnrelated1} Sen Maitra, Durjoy \& Bhattacharya, Ujjwal \& Parui, Swapan. \ (2015). "CNN based common approach to handwritten character recognition of multiple scripts." 1021-1025. 10.1109\/ICDAR.2015.7333916.

  \bibitem[Elleuch et al. (2016)]{cnnrelated2} Elleuch, Mohamed \& Maalej, Rania. \ (2016). "A New Design Based-SVM of the CNN Classifier Architecture with Dropout for Offline Arabic Handwritten Recognition." Procedia Computer Science. 80. 1712-1723. 10.1016\/j.procs.2016.05.512.

  \bibitem[Zhong et al. (2015)]{cnnrelated3} Zhong, Zhuoyao \& Jin, Lianwen \& Xie, Zecheng. \ (2015)." High performance offline handwritten Chinese character recognition using GoogLeNet and directional feature maps." 846-850. 10.1109\/ICDAR.2015.7333881.

  \bibitem[Ding et al. (2017)]{cnnrelated3} Ding, Haisong \& Chen, Kai \& Yuan, Ye \& Cai, Meng \& Sun, Lei \& Liang, Sen \& Huo, Qiang. \ (2017). "A Compact CNN-DBLSTM Based Character Model for Offline Handwriting Recognition with Tucker Decomposition." 507-512. 10.1109\/ICDAR.2017.89.

  \bibitem[Govindarajan (2013)]{imp1} Govindarajan, M.. (2013). "Evaluation of Ensemble Classifiers for Handwriting Recognition." International Journal of Modern Education and Computer Science. 5. 11-20. 10.5815\/ijmecs.2013.11.02.

  \bibitem[Mahreen et al. (2017)]{imp2} Ahmed, Mahreen \& Rasool, Asma \& Afzal, Hammad \& Siddiqi, Imran. \ (2017). "Improving Handwriting based Gender Classification using Ensemble Classifiers." Expert Systems with Applications. 85. 10.1016\/j.eswa.2017.05.033.

  \bibitem[Ramy et al. (2009)]{imp3} Mohamad, Ramy \& Likforman-Sulem, Laurence \& Mokbel, Chafic. \ (2009). "Combining Slanted-Frame Classifiers for Improved HMM-Based Arabic Handwriting Recognition." IEEE transactions on pattern analysis and machine intelligence. 31. 1165-77. 10.1109\/TPAMI.2008.136.

  \bibitem[Sivan (2019)]{augment} Keret, Sivan \ (2019) \ “Transductive Learning for Reading Handwritten Tibetan Manuscripts.”

  \bibitem[Wigington et al. (2017)]{augment1} C. Wigington \& S. Stewart \& B. Davis \& B. Barrett \& B. Price \& S. Cohen \ (2017) \ "Data Augmentation for Recognition of Handwritten Words and Lines Using a CNN-LSTM Network," 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), Kyoto, 2017, pp. 639-645. doi: 10.1109\/ICDAR.2017.110
  
  % Related work (new)
  \bibitem[Barend et al. (2018)]{custom1} Harris, Barend \& Moghaddam, Mansureh S \& Kang, Duseok \& Bae, Inpyo \& Kim, Euiseok \& Min, Hyemi \& Cho, Hansu \& Kim, Sukjin \& Egger, Bernhard \& Ha, Soonhoi and others \ (2018) "Architectures and algorithms for user customization of CNNs." Proceedings of the 23rd Asia and South Pacific Design Automation Conference. IEEE Press, 2018.

  \bibitem[Boyu et al. (2018)]{custom2} Zhang, Boyu \& Azadeh Davoodi \& Yu-Hen Hu. \ (2018) \ "A Mixture of Expert Approach for Low-Cost Customization of Deep Neural Networks." arXiv preprint arXiv:1811.00056 (2018).

  \bibitem[Xu et al. (2015)]{custom3} Zhang, Xu \& Yu, Felix Xinnan \& Chang, Shih-Fu \& Wang, Shengjin \ (2015) \ "Deep transfer network: Unsupervised domain adaptation." arXiv preprint arXiv:1503.00591.

  \bibitem[Choi et al. (2019)]{custom4} Seungkyu Choi \& Jaekang Shin \& Yeongjae Choi \& Lee-Sup Kim (2019) \ "An Optimized Design Technique of Low-bit Neural Network Training for Personalization on IoT Devices." In Proceedings of the 56th Annual Design Automation Conference 2019 (DAC '19). ACM, New York, NY, USA, Article 191, 6 pages. DOI: https://doi.org/10.1145/3316781.3317769

  \bibitem[Yann et al. (2019)]{custom5} Soullard Yann \& Wassim Swaileh \& Pierrick Tranouez \& Thierry Paquet \& Clément \& Châtelain \ (2019) \ “Improving text recognition using optical and language model writer adaptation.” (2019).

  \bibitem[Prasad et al. (2010)]{adapt1} Prasad, Rohit \& Bhardwaj, Anurag \& Subramanian, Krishna \& Cao, Huaigu \& Natarajan, Premkumar \ (2010) \ "Stochastic Segment Model Adaptation for Offline Handwriting Recognition." 1993-1996. 10.1109\/ICPR.2010.491. 

  \bibitem[Hidetoshi et al. (2009)]{adapt2} Miyao, Hidetoshi \& Maruyama, Minoru \ (2009) \ "Writer Adaptation for Online Handwriting Recognition System Using Virtual Examples." 1156-1160. 10.1109\/ICDAR.2009.174. 

  \bibitem[Connell et al. (2002)]{adapt3} S. D. Connell \&  A. K. Jain \ (2002) \ "Writer adaptation for online handwriting recognition," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 3, pp. 329-346, March 2002. doi: 10.1109\/34.990135

  \bibitem[Kozielski et al. (2013)]{adapt4} Kozielski, Michal \& Doetsch, Patrick \& Ney, Hermann \ (2013) \ "Improvements in RWTH's System for Off-Line Handwriting Recognition." Proceedings of the International Conference on Document Analysis and Recognition, ICDAR. 935-939. 10.1109\/ICDAR.2013.190. 

  \bibitem[Ball et al. (2008)]{adapt5} Ball, Gregory \& Srihari, Sargur \ (2008) \ "Prototype Integration in Off-Line Handwriting Recognition Adaptation"

  \bibitem[Nosary et al. (2004)]{nosary3} Nosary, Ali \& Heutte, Laurent \& Paquet, Thierry \ (2004) \ "Unsupervised writer adaptation applied to handwritten text recognition." Pattern Recognition. 37. 385-388. 10.1016\/S0031-3203(03)00185-7. 

  \bibitem[Nosary et al. (2002)]{nosary2} Nosary, Ali \& Paquet, Thierry \& Heutte, Laurent \& Bensefia, Ameur \ (2002) \ "Handwritten text recognition through writer adaptation." Proceedings - International Workshop on Frontiers in Handwriting Recognition, IWFHR. 363 - 368. 10.1109\/IWFHR.2002.1030937. 

  \bibitem[Nosary et al. (1999)]{nosary1} Nosary, Ali \& Heutte, Laurent \& Paquet, Thierry \& Lecourtier, Yves \ (1999) \ "Defining Writer's Invariants to Adapt the Recognition Task" 765-768. 10.1109\/ICDAR.1999.791900. 

  \bibitem[Vinciarelli et al. (2002)]{adapt6} Vinciarelli, Alessandro \& Bengio, Samy \ (2002) \ "Writer adaptation techniques in HMM based Off-Line Cursive Script Recognition." Pattern Recognition Letters. 23. 905-916. 10.1016\/S0167-8655(02)00021-1. 

  \bibitem[Yang et al. (2018)]{adapt7} Yang, Hong-Ming \& Zhang, Xu-Yao \& Yin, Fei \& Sun, Jun \& Liu, Cheng-Lin \ (2018) \ "Deep Transfer Mapping for Unsupervised Writer Adaptation." 2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR). IEEE, 2018.

  \bibitem[Yang et al. (2018)]{cnn1} H. Yang \& X. Zhang \& F. Yin \& J. Sun \& C. Liu \ (2018) \ "Deep Transfer Mapping for Unsupervised Writer Adaptation," 2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR), Niagara Falls, NY, 2018, pp. 151-156.

  \bibitem[Yang et al. (2016)]{cnn2} H. Yang \& X. Zhang \& F. Yin \& Z. Luo \& C. Liu \ (2016)  "Unsupervised Adaptation of Neural Networks for Chinese Handwriting Recognition," 2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR), Shenzhen, 2016, pp. 512-517. doi: 10.1109\/ICFHR.2016.0100

  \bibitem[Wang et al. (2016)]{cnn3} Z. Wang \& J. Du \ (2016) \  "Writer Code Based Adaptation of Deep Neural Network for Offline Handwritten Chinese Text Recognition," 2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR), Shenzhen, 2016, pp. 548-553. doi: 10.1109\/ICFHR.2016.0106

  \bibitem[Zhang et al. (2018)]{cnn4} Yaping Zhang \& Shan Liang \& Shuai Nie \& Wenju Liu \& Shouye Peng \ (2018) \ "Robust offline handwritten character recognition through exploring writer-independent features under the guidance of printed data", Pattern Recognition Letters, Volume 106, 2018, Pages 20-26, ISSN 0167-8655, https://doi.org/10.1016/j.patrec.2018.02.006.

  \bibitem[Xu-Yao et al. (2013)]{cnn0} Zhang, Xu-Yao \& Liu, Cheng-Lin \ (2013) \ "Writer Adaptation with Style Transfer Mapping." IEEE transactions on pattern analysis and machine intelligence. 35. 1773-87. 10.1109\/TPAMI.2012.239. 
\end{thebibliography}

\end{document}